diff --git a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp
index 1f0432196a2..8d9831f996d 100644
--- a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp
@@ -424,7 +424,10 @@ bool SelectionDAGISel::runOnMachineFunction(MachineFunction &mf) {
   // FIXME: This is a horrible hack and should be processed via
   // codegen looking at the optimization level explicitly when
   // it wants to look at it.
-  TM.resetTargetOptions(Fn);
+  
+  //TODO is this correct
+  //TM.resetTargetOptions(Fn);
+  
   // Reset OptLevel to None for optnone functions.
   CodeGenOpt::Level NewOptLevel = OptLevel;
   if (OptLevel != CodeGenOpt::None && skipFunction(Fn))
diff --git a/mlir/lib/Conversion/GPUCommon/CMakeLists.txt b/mlir/lib/Conversion/GPUCommon/CMakeLists.txt
index 9cb6038b302..1b1743d0a74 100644
--- a/mlir/lib/Conversion/GPUCommon/CMakeLists.txt
+++ b/mlir/lib/Conversion/GPUCommon/CMakeLists.txt
@@ -25,6 +25,7 @@ add_mlir_conversion_library(MLIRGPUToGPURuntimeTransforms
   LINK_COMPONENTS
   Core
   MC
+  CODEGEN
   ${AMDGPU_LIBS}
   ${NVPTX_LIBS}
 
diff --git a/mlir/lib/Conversion/GPUCommon/ConvertKernelFuncToBlob.cpp b/mlir/lib/Conversion/GPUCommon/ConvertKernelFuncToBlob.cpp
index a42df092b15..07b030f8668 100644
--- a/mlir/lib/Conversion/GPUCommon/ConvertKernelFuncToBlob.cpp
+++ b/mlir/lib/Conversion/GPUCommon/ConvertKernelFuncToBlob.cpp
@@ -35,6 +35,9 @@
 #include "llvm/Support/TargetSelect.h"
 #include "llvm/Target/TargetMachine.h"
 
+#include "llvm/CodeGen/CommandFlags.h"
+#include "llvm/Analysis/TargetTransformInfo.h"
+
 using namespace mlir;
 
 namespace {
@@ -120,6 +123,9 @@ GpuKernelToBlobPass::translateModuleToISA(llvm::Module &module,
 OwnedBlob GpuKernelToBlobPass::convertModuleToBlob(llvm::Module &llvmModule,
                                                    Location loc,
                                                    StringRef name) {
+  
+  llvm::TargetOptions options = llvm::codegen::InitTargetOptionsFromCodeGenFlags();
+
   std::unique_ptr<llvm::TargetMachine> targetMachine;
   {
     std::string error;
@@ -130,7 +136,7 @@ OwnedBlob GpuKernelToBlobPass::convertModuleToBlob(llvm::Module &llvmModule,
       return {};
     }
     targetMachine.reset(target->createTargetMachine(triple.str(), targetChip,
-                                                    features, {}, {}));
+                                                    features, options, {}));
   }
 
   llvmModule.setDataLayout(targetMachine->createDataLayout());
diff --git a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
index 8aa843308cf..2cbf10460d2 100644
--- a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
+++ b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
@@ -100,12 +100,12 @@ protected:
       llvmVoidType,
       {
           llvmPointerType,        /* void* f */
-          llvmIntPtrType,         /* intptr_t gridXDim */
-          llvmIntPtrType,         /* intptr_t gridyDim */
-          llvmIntPtrType,         /* intptr_t gridZDim */
-          llvmIntPtrType,         /* intptr_t blockXDim */
-          llvmIntPtrType,         /* intptr_t blockYDim */
-          llvmIntPtrType,         /* intptr_t blockZDim */
+          llvmInt32Type,         /* intptr_t gridXDim */
+          llvmInt32Type,         /* intptr_t gridyDim */
+          llvmInt32Type,         /* intptr_t gridZDim */
+          llvmInt32Type,         /* intptr_t blockXDim */
+          llvmInt32Type,         /* intptr_t blockYDim */
+          llvmInt32Type,         /* intptr_t blockZDim */
           llvmInt32Type,          /* unsigned int sharedMemBytes */
           llvmPointerType,        /* void *hstream */
           llvmPointerPointerType, /* void **kernelParams */
@@ -123,6 +123,15 @@ protected:
       {llvmIntPtrType /* intptr_t rank */,
        llvmPointerType /* void *memrefDesc */,
        llvmIntPtrType /* intptr_t elementSizeBytes */}};
+  FunctionCallBuilder memAllocCallBuilder = {
+      "mgpuMemAlloc",
+      llvmVoidType,
+      {
+          llvmPointerPointerType, /* void **ptr */
+          llvmInt32Type           /* int32 sizeBytes */
+      }};
+  FunctionCallBuilder memFreeCallBuilder = {
+      "mgpuMemFree", llvmVoidType, {llvmPointerType /* void *ptr */}};
 };
 
 /// A rewrite patter to convert gpu.host_register operations into a GPU runtime
@@ -185,17 +194,43 @@ class EraseGpuModuleOpPattern : public OpRewritePattern<gpu::GPUModuleOp> {
   }
 };
 
+class ReplaceMallocAndFreePattern
+    : public ConvertOpToGpuRuntimeCallPattern<LLVM::CallOp> {
+public:
+  ReplaceMallocAndFreePattern(LLVMTypeConverter &typeConverter)
+      : ConvertOpToGpuRuntimeCallPattern<LLVM::CallOp>(typeConverter) {}
+
+  LogicalResult
+  matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+                  ConversionPatternRewriter &rewriter) const override;
+};
+
 } // namespace
 
 void GpuToLLVMConversionPass::runOnOperation() {
-  LLVMTypeConverter converter(&getContext());
+  LowerToLLVMOptions options = {/*useBarePtrCallConv =*/false,
+                                /*emitCWrappers =*/true,
+                                /*indexBitwidth =*/32,
+                                /*useAlignedAlloc =*/false};
+  LLVMTypeConverter converter(&getContext(), options);
+
   OwningRewritePatternList patterns;
   populateStdToLLVMConversionPatterns(converter, patterns);
   populateGpuToLLVMConversionPatterns(converter, patterns, gpuBinaryAnnotation);
 
   LLVMConversionTarget target(getContext());
+  target.addDynamicallyLegalOp<LLVM::CallOp>([](LLVM::CallOp callOp) {
+    return callOp.callee().getValueOr("") != "malloc" &&
+           callOp.callee().getValueOr("") != "free";
+  });
   if (failed(applyPartialConversion(getOperation(), target, patterns)))
     signalPassFailure();
+
+  // Erase the malloc and free function declarations if they are unused.
+  if (auto *malloc = getOperation().lookupSymbol("malloc"))
+    malloc->erase();
+  if (auto *free = getOperation().lookupSymbol("free"))
+    free->erase();
 }
 
 LLVM::CallOp FunctionCallBuilder::create(Location loc, OpBuilder &builder,
@@ -280,7 +315,7 @@ Value ConvertLaunchFuncOpToGpuRuntimeCallPattern::generateParamsArray(
     auto index = builder.create<LLVM::ConstantOp>(
         loc, llvmInt32Type, builder.getI32IntegerAttr(en.index()));
     auto fieldPtr =
-        builder.create<LLVM::GEPOp>(loc, structType.getPointerTo(), structPtr,
+        builder.create<LLVM::GEPOp>(loc, argumentTypes[en.index()].getPointerTo(), structPtr,
                                     ArrayRef<Value>{zero, index.getResult()});
     builder.create<LLVM::StoreOp>(loc, en.value(), fieldPtr);
     auto elementPtr = builder.create<LLVM::GEPOp>(loc, llvmPointerPointerType,
@@ -385,6 +420,32 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
   return success();
 }
 
+LogicalResult ReplaceMallocAndFreePattern::matchAndRewrite(
+    Operation *op, ArrayRef<Value> operands,
+    ConversionPatternRewriter &rewriter) const {
+  auto callOp = cast<LLVM::CallOp>(op);
+  auto loc = callOp.getLoc();
+
+  // Replace all memory allocations by GPU memory allocations or frees.
+  if (callOp.callee().getValue() == "malloc") {
+    auto one = rewriter.create<LLVM::ConstantOp>(loc, llvmInt64Type,
+                                                 rewriter.getI64IntegerAttr(1));
+    auto allocPtr =
+        rewriter.create<LLVM::AllocaOp>(loc, llvmPointerPointerType, one, 0);
+    memAllocCallBuilder.create(loc, rewriter, ArrayRef<Value>{allocPtr, callOp.getOperand(0)});
+    auto loadOp = rewriter.create<LLVM::LoadOp>(loc, llvmPointerType, allocPtr);
+    rewriter.replaceOp(op, loadOp.getResult());
+    return success();
+  }
+  if (callOp.callee().getValue() == "free") {
+    memFreeCallBuilder.create(loc, rewriter,
+                              ArrayRef<Value>{callOp.getOperand(0)});
+    rewriter.eraseOp(op);
+    return success();
+  }
+  return failure();
+}
+
 std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
 mlir::createGpuToLLVMConversionPass(StringRef gpuBinaryAnnotation) {
   return std::make_unique<GpuToLLVMConversionPass>(gpuBinaryAnnotation);
@@ -396,5 +457,6 @@ void mlir::populateGpuToLLVMConversionPatterns(
   patterns.insert<ConvertHostRegisterOpToGpuRuntimeCallPattern>(converter);
   patterns.insert<ConvertLaunchFuncOpToGpuRuntimeCallPattern>(
       converter, gpuBinaryAnnotation);
+  patterns.insert<ReplaceMallocAndFreePattern>(converter);
   patterns.insert<EraseGpuModuleOpPattern>(&converter.getContext());
 }
diff --git a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
index 517fc9fc18f..bd9be5e84e8 100644
--- a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
@@ -14,6 +14,8 @@
 
 #include <cassert>
 #include <numeric>
+#include <iostream>
+#include <iomanip>
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
@@ -47,14 +49,25 @@ extern "C" CUfunction mgpuModuleGetFunction(CUmodule module, const char *name) {
 // The wrapper uses intptr_t instead of CUDA's unsigned int to match
 // the type of MLIR's index type. This avoids the need for casts in the
 // generated MLIR code.
-extern "C" void mgpuLaunchKernel(CUfunction function, intptr_t gridX,
-                                 intptr_t gridY, intptr_t gridZ,
-                                 intptr_t blockX, intptr_t blockY,
-                                 intptr_t blockZ, int32_t smem, CUstream stream,
+extern "C" void mgpuLaunchKernel(CUfunction function, int32_t gridX,
+                                 int32_t gridY, int32_t gridZ,
+                                 int32_t blockX, int32_t blockY,
+                                 int32_t blockZ, int32_t smem, CUstream stream,
                                  void **params, void **extra) {
+  CUevent start, stop;
+  CUDA_REPORT_IF_ERROR(cuEventCreate(&start, CU_EVENT_DEFAULT));
+  CUDA_REPORT_IF_ERROR(cuEventCreate(&stop, CU_EVENT_DEFAULT));
+  
+  CUDA_REPORT_IF_ERROR(cuEventRecord(start, stream));
   CUDA_REPORT_IF_ERROR(cuLaunchKernel(function, gridX, gridY, gridZ, blockX,
-                                      blockY, blockZ, smem, stream, params,
-                                      extra));
+                                       blockY, blockZ, smem, stream, params,
+                                       extra));
+  CUDA_REPORT_IF_ERROR(cuEventRecord(stop, stream));
+
+  CUDA_REPORT_IF_ERROR(cuEventSynchronize(stop));
+  float duration = 0.0;
+  CUDA_REPORT_IF_ERROR(cuEventElapsedTime(&duration, start, stop));
+  std::cout << std::setprecision(5) << "-> kernel time [ms]: " << duration << "\n";
 }
 
 extern "C" CUstream mgpuStreamCreate() {
@@ -67,6 +80,14 @@ extern "C" void mgpuStreamSynchronize(CUstream stream) {
   CUDA_REPORT_IF_ERROR(cuStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(CUdeviceptr *ptr, uint32_t size) {
+  CUDA_REPORT_IF_ERROR(cuMemAlloc(ptr, size));
+}
+
+extern "C" void mgpuMemFree(CUdeviceptr ptr) {
+  CUDA_REPORT_IF_ERROR(cuMemFree(ptr));
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the CUDA runtime. Helpful until we have
diff --git a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
index 9184c9fa20f..e1804d3928a 100644
--- a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
@@ -14,6 +14,8 @@
 
 #include <cassert>
 #include <numeric>
+#include <iostream>
+#include <iomanip>
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
@@ -47,16 +49,27 @@ extern "C" hipFunction_t mgpuModuleGetFunction(hipModule_t module,
 // The wrapper uses intptr_t instead of ROCM's unsigned int to match
 // the type of MLIR's index type. This avoids the need for casts in the
 // generated MLIR code.
-extern "C" void mgpuLaunchKernel(hipFunction_t function, intptr_t gridX,
-                                 intptr_t gridY, intptr_t gridZ,
-                                 intptr_t blockX, intptr_t blockY,
-                                 intptr_t blockZ, int32_t smem,
+extern "C" void mgpuLaunchKernel(hipFunction_t function, int32_t gridX,
+                                 int32_t gridY, int32_t gridZ,
+                                 int32_t blockX, int32_t blockY,
+                                 int32_t blockZ, int32_t smem,
                                  hipStream_t stream, void **params,
                                  void **extra) {
+  hipEvent_t start, stop;
+  HIP_REPORT_IF_ERROR(hipEventCreate(&start));
+  HIP_REPORT_IF_ERROR(hipEventCreate(&stop));
+  
+  HIP_REPORT_IF_ERROR(hipEventRecord(start, stream));
   HIP_REPORT_IF_ERROR(hipModuleLaunchKernel(function, gridX, gridY, gridZ,
                                             blockX, blockY, blockZ, smem,
                                             stream, params, extra));
-}
+  HIP_REPORT_IF_ERROR(hipEventRecord(stop, stream));
+
+  HIP_REPORT_IF_ERROR(hipEventSynchronize(stop));
+  float duration = 0.0;
+  HIP_REPORT_IF_ERROR(hipEventElapsedTime(&duration, start, stop));
+  std::cout << std::setprecision(5) << "-> kernel time [ms]: " << duration << "\n";
+ }
 
 extern "C" void *mgpuStreamCreate() {
   hipStream_t stream = nullptr;
@@ -68,6 +81,14 @@ extern "C" void mgpuStreamSynchronize(hipStream_t stream) {
   return HIP_REPORT_IF_ERROR(hipStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(hipDeviceptr_t *ptr, uint32_t size) {
+  HIP_REPORT_IF_ERROR(hipMalloc(ptr, size));
+}
+
+extern "C" void mgpuMemFree(hipDeviceptr_t ptr) {
+  HIP_REPORT_IF_ERROR(hipFree(ptr));
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the ROCM runtime. Helpful until we have
@@ -107,18 +128,18 @@ void mgpuMemGetDevicePointer(T *hostPtr, T **devicePtr) {
       hipHostGetDevicePointer((void **)devicePtr, hostPtr, /*flags=*/0));
 }
 
-extern "C" StridedMemRefType<float, 1>
-mgpuMemGetDeviceMemRef1dFloat(float *allocated, float *aligned, int64_t offset,
-                              int64_t size, int64_t stride) {
-  float *devicePtr = nullptr;
-  mgpuMemGetDevicePointer(aligned, &devicePtr);
-  return {devicePtr, devicePtr, offset, {size}, {stride}};
-}
-
-extern "C" StridedMemRefType<int32_t, 1>
-mgpuMemGetDeviceMemRef1dInt32(int32_t *allocated, int32_t *aligned,
-                              int64_t offset, int64_t size, int64_t stride) {
-  int32_t *devicePtr = nullptr;
-  mgpuMemGetDevicePointer(aligned, &devicePtr);
-  return {devicePtr, devicePtr, offset, {size}, {stride}};
-}
+// extern "C" StridedMemRefType<float, 1>
+// mgpuMemGetDeviceMemRef1dFloat(float *allocated, float *aligned, int64_t offset,
+//                               int64_t size, int64_t stride) {
+//   float *devicePtr = nullptr;
+//   mgpuMemGetDevicePointer(aligned, &devicePtr);
+//   return {devicePtr, devicePtr, offset, {size}, {stride}};
+// }
+
+// extern "C" StridedMemRefType<int32_t, 1>
+// mgpuMemGetDeviceMemRef1dInt32(int32_t *allocated, int32_t *aligned,
+//                               int64_t offset, int64_t size, int64_t stride) {
+//   int32_t *devicePtr = nullptr;
+//   mgpuMemGetDevicePointer(aligned, &devicePtr);
+//   return {devicePtr, devicePtr, offset, {size}, {stride}};
+// }
