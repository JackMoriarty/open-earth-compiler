diff --git a/mlir/include/mlir/Conversion/GPUCommon/GPUCommonPass.h b/mlir/include/mlir/Conversion/GPUCommon/GPUCommonPass.h
index 61d62a3ef78c..d1fbdc846e94 100644
--- a/mlir/include/mlir/Conversion/GPUCommon/GPUCommonPass.h
+++ b/mlir/include/mlir/Conversion/GPUCommon/GPUCommonPass.h
@@ -8,6 +8,7 @@
 #ifndef MLIR_CONVERSION_GPUCOMMON_GPUCOMMONPASS_H_
 #define MLIR_CONVERSION_GPUCOMMON_GPUCOMMONPASS_H_
 
+#include "mlir/Conversion/StandardToLLVM/ConvertStandardToLLVMPass.h"
 #include "mlir/Support/LLVM.h"
 #include "llvm/IR/Module.h"
 #include <vector>
@@ -45,7 +46,9 @@ using LoweringCallback = std::function<std::unique_ptr<llvm::Module>(
 /// instead uses a small wrapper library that exports a stable and conveniently
 /// typed ABI on top of GPU runtimes such as CUDA or ROCm (HIP).
 std::unique_ptr<OperationPass<ModuleOp>>
-createGpuToLLVMConversionPass(StringRef gpuBinaryAnnotation = "");
+createGpuToLLVMConversionPass(StringRef gpuBinaryAnnotation = "",
+                              const LowerToLLVMOptions &options =
+                                  LowerToLLVMOptions::getDefaultOptions());
 
 /// Collect a set of patterns to convert from the GPU dialect to LLVM.
 void populateGpuToLLVMConversionPatterns(LLVMTypeConverter &converter,
diff --git a/mlir/include/mlir/Conversion/Passes.td b/mlir/include/mlir/Conversion/Passes.td
index aa228784e48a..61e4ee8b5498 100644
--- a/mlir/include/mlir/Conversion/Passes.td
+++ b/mlir/include/mlir/Conversion/Passes.td
@@ -109,6 +109,22 @@ def GpuToLLVMConversionPass : Pass<"gpu-to-llvm", "ModuleOp"> {
   let options = [
     Option<"gpuBinaryAnnotation", "gpu-binary-annotation", "std::string",
            "", "Annotation attribute string for GPU binary">,
+    Option<"useAlignedAlloc", "use-aligned-alloc", "bool", /*default=*/"false",
+           "Use aligned_alloc in place of malloc for heap allocations">,
+    Option<"useBarePtrCallConv", "use-bare-ptr-memref-call-conv", "bool",
+           /*default=*/"false",
+           "Replace FuncOp's MemRef arguments with bare pointers to the MemRef "
+           "element types">,
+    Option<"emitCWrappers", "emit-c-wrappers", "bool", /*default=*/"false",
+           "Emit wrappers for C-compatible pointer-to-struct memref "
+           "descriptors">,
+    Option<"indexBitwidth", "index-bitwidth", "unsigned",
+           /*default=kDeriveIndexBitwidthFromDataLayout*/"0",
+           "Bitwidth of the index type, 0 to use size of machine word">,
+    Option<"dataLayout", "data-layout", "std::string",
+           /*default=*/"\"\"",
+           "String description (LLVM format) of the data layout that is "
+           "expected on the produced module">
   ];
 }
 
diff --git a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
index cee1d7ba20e3..7fd9f2cd95d3 100644
--- a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
+++ b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
@@ -41,9 +41,17 @@ namespace {
 class GpuToLLVMConversionPass
     : public GpuToLLVMConversionPassBase<GpuToLLVMConversionPass> {
 public:
-  GpuToLLVMConversionPass(StringRef gpuBinaryAnnotation) {
+  GpuToLLVMConversionPass(StringRef gpuBinaryAnnotation,
+                          bool useBarePtrCallConv, bool emitCWrappers,
+                          unsigned indexBitwidth, bool useAlignedAlloc,
+                          const llvm::DataLayout &dataLayout) {
     if (!gpuBinaryAnnotation.empty())
       this->gpuBinaryAnnotation = gpuBinaryAnnotation.str();
+    this->useBarePtrCallConv = useBarePtrCallConv;
+    this->emitCWrappers = emitCWrappers;
+    this->indexBitwidth = indexBitwidth;
+    this->useAlignedAlloc = useAlignedAlloc;
+    this->dataLayout = dataLayout.getStringRepresentation();
   }
 
   // Run the dialect converter on the module.
@@ -56,7 +64,7 @@ public:
                       ArrayRef<Type> argumentTypes)
       : functionName(functionName),
         functionType(LLVM::LLVMFunctionType::get(returnType, argumentTypes)) {}
-  LLVM::CallOp create(Location loc, OpBuilder &builder,
+  LLVM::CallOp create(Location loc, OpBuilder &builder, unsigned indexBitwidth,
                       ArrayRef<Value> arguments) const;
 
 private:
@@ -291,7 +299,10 @@ private:
 } // namespace
 
 void GpuToLLVMConversionPass::runOnOperation() {
-  LLVMTypeConverter converter(&getContext());
+  LowerToLLVMOptions options = {useBarePtrCallConv, emitCWrappers,
+                                indexBitwidth, useAlignedAlloc,
+                                llvm::DataLayout(this->dataLayout)};
+  LLVMTypeConverter converter(&getContext(), options);
   OwningRewritePatternList patterns;
   populateStdToLLVMConversionPatterns(converter, patterns);
   populateGpuToLLVMConversionPatterns(converter, patterns, gpuBinaryAnnotation);
@@ -303,6 +314,7 @@ void GpuToLLVMConversionPass::runOnOperation() {
 }
 
 LLVM::CallOp FunctionCallBuilder::create(Location loc, OpBuilder &builder,
+                                         unsigned indexBitwidth,
                                          ArrayRef<Value> arguments) const {
   auto module = builder.getBlock()->getParent()->getParentOfType<ModuleOp>();
   auto function = [&] {
@@ -311,9 +323,33 @@ LLVM::CallOp FunctionCallBuilder::create(Location loc, OpBuilder &builder,
     return OpBuilder(module.getBody()->getTerminator())
         .create<LLVM::LLVMFuncOp>(loc, functionName, functionType);
   }();
+  // Cast index arguments to the bitwidth used by the runtime wrapper functions.
+  SmallVector<Value, 4> castedArguments;
+  castedArguments.reserve(arguments.size());
+  llvm::transform(
+      llvm::zip(arguments,
+                const_cast<LLVM::LLVMFunctionType &>(functionType).getParams()),
+      std::back_inserter(castedArguments), [&](const auto &pair) -> Value {
+        auto value = std::get<0>(pair);
+        auto paramType = std::get<1>(pair);
+        if (value.getType().isIndex() &&
+            paramType.getIntOrFloatBitWidth() > indexBitwidth) {
+          return builder.create<LLVM::SExtOp>(loc, paramType, value);
+        }
+        if (value.getType().isInteger(indexBitwidth) &&
+            paramType.getIntOrFloatBitWidth() >
+                value.getType().getIntOrFloatBitWidth()) {
+          return value.getType().isUnsignedInteger()
+                     ? builder.create<LLVM::ZExtOp>(loc, paramType, value)
+                           .getResult()
+                     : builder.create<LLVM::SExtOp>(loc, paramType, value)
+                           .getResult();
+        }
+        return value;
+      });
   return builder.create<LLVM::CallOp>(
       loc, const_cast<LLVM::LLVMFunctionType &>(functionType).getReturnType(),
-      builder.getSymbolRefAttr(function), arguments);
+      builder.getSymbolRefAttr(function), castedArguments);
 }
 
 // Returns whether all operands are of LLVM type.
@@ -348,6 +384,7 @@ LogicalResult ConvertHostRegisterOpToGpuRuntimeCallPattern::matchAndRewrite(
     return failure();
 
   Location loc = op->getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
 
   auto memRefType = hostRegisterOp.value().getType();
   auto elementType = memRefType.cast<UnrankedMemRefType>().getElementType();
@@ -356,7 +393,7 @@ LogicalResult ConvertHostRegisterOpToGpuRuntimeCallPattern::matchAndRewrite(
   auto arguments = getTypeConverter()->promoteOperands(loc, op->getOperands(),
                                                        operands, rewriter);
   arguments.push_back(elementSize);
-  hostRegisterCallBuilder.create(loc, rewriter, arguments);
+  hostRegisterCallBuilder.create(loc, rewriter, indexBitwidth, arguments);
 
   rewriter.eraseOp(op);
   return success();
@@ -373,6 +410,7 @@ LogicalResult ConvertAllocOpToGpuRuntimeCallPattern::matchAndRewrite(
     return failure();
 
   auto loc = allocOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
   auto adaptor = gpu::AllocOpAdaptor(operands, allocOp->getAttrDictionary());
 
   // Get shape of the memref as values: static sizes are constant
@@ -388,7 +426,8 @@ LogicalResult ConvertAllocOpToGpuRuntimeCallPattern::matchAndRewrite(
   Type elementPtrType = this->getElementPtrType(memRefType);
   auto stream = adaptor.asyncDependencies().front();
   Value allocatedPtr =
-      allocCallBuilder.create(loc, rewriter, {sizeBytes, stream}).getResult(0);
+      allocCallBuilder.create(loc, rewriter, indexBitwidth, {sizeBytes, stream})
+          .getResult(0);
   allocatedPtr =
       rewriter.create<LLVM::BitcastOp>(loc, elementPtrType, allocatedPtr);
 
@@ -412,6 +451,7 @@ LogicalResult ConvertDeallocOpToGpuRuntimeCallPattern::matchAndRewrite(
     return failure();
 
   Location loc = deallocOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
 
   auto adaptor =
       gpu::DeallocOpAdaptor(operands, deallocOp->getAttrDictionary());
@@ -419,7 +459,7 @@ LogicalResult ConvertDeallocOpToGpuRuntimeCallPattern::matchAndRewrite(
       MemRefDescriptor(adaptor.memref()).allocatedPtr(rewriter, loc);
   auto casted = rewriter.create<LLVM::BitcastOp>(loc, llvmPointerType, pointer);
   Value stream = adaptor.asyncDependencies().front();
-  deallocCallBuilder.create(loc, rewriter, {casted, stream});
+  deallocCallBuilder.create(loc, rewriter, indexBitwidth, {casted, stream});
 
   rewriter.replaceOp(deallocOp, {stream});
   return success();
@@ -437,11 +477,14 @@ LogicalResult ConvertWaitOpToGpuRuntimeCallPattern::matchAndRewrite(
     return rewriter.notifyMatchFailure(waitOp, "Cannot convert async op.");
 
   Location loc = waitOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
 
   for (auto asyncDependency : operands)
-    streamSynchronizeCallBuilder.create(loc, rewriter, {asyncDependency});
+    streamSynchronizeCallBuilder.create(loc, rewriter, indexBitwidth,
+                                        {asyncDependency});
   for (auto asyncDependency : operands)
-    streamDestroyCallBuilder.create(loc, rewriter, {asyncDependency});
+    streamDestroyCallBuilder.create(loc, rewriter, indexBitwidth,
+                                    {asyncDependency});
 
   rewriter.eraseOp(waitOp);
   return success();
@@ -460,6 +503,7 @@ LogicalResult ConvertWaitAsyncOpToGpuRuntimeCallPattern::matchAndRewrite(
     return rewriter.notifyMatchFailure(waitOp, "Can only convert async op.");
 
   Location loc = waitOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
 
   auto insertionPoint = rewriter.saveInsertionPoint();
   SmallVector<Value, 1> events;
@@ -472,17 +516,21 @@ LogicalResult ConvertWaitAsyncOpToGpuRuntimeCallPattern::matchAndRewrite(
       // which is late and therefore misses parallelism, but still valid.
       rewriter.setInsertionPointToStart(waitOp->getBlock());
     }
-    auto event = eventCreateCallBuilder.create(loc, rewriter, {}).getResult(0);
+    auto event = eventCreateCallBuilder.create(loc, rewriter, indexBitwidth, {})
+                     .getResult(0);
     auto stream = std::get<1>(pair);
-    eventRecordCallBuilder.create(loc, rewriter, {event, stream});
+    eventRecordCallBuilder.create(loc, rewriter, indexBitwidth,
+                                  {event, stream});
     events.push_back(event);
   }
   rewriter.restoreInsertionPoint(insertionPoint);
-  auto stream = streamCreateCallBuilder.create(loc, rewriter, {}).getResult(0);
+  auto stream = streamCreateCallBuilder.create(loc, rewriter, indexBitwidth, {})
+                    .getResult(0);
   for (auto event : events)
-    streamWaitEventCallBuilder.create(loc, rewriter, {stream, event});
+    streamWaitEventCallBuilder.create(loc, rewriter, indexBitwidth,
+                                      {stream, event});
   for (auto event : events)
-    eventDestroyCallBuilder.create(loc, rewriter, {event});
+    eventDestroyCallBuilder.create(loc, rewriter, indexBitwidth, {event});
   rewriter.replaceOp(waitOp, {stream});
 
   return success();
@@ -601,6 +649,7 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
         launchOp, "Cannot convert non-async op with async dependencies.");
 
   Location loc = launchOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
 
   // Create an LLVM global with CUBIN extracted from the kernel annotation and
   // obtain a pointer to the first byte in it.
@@ -622,25 +671,27 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
       LLVM::createGlobalString(loc, rewriter, nameBuffer.str(),
                                binaryAttr.getValue(), LLVM::Linkage::Internal);
 
-  auto module = moduleLoadCallBuilder.create(loc, rewriter, data);
+  auto module =
+      moduleLoadCallBuilder.create(loc, rewriter, indexBitwidth, data);
   // Get the function from the module. The name corresponds to the name of
   // the kernel function.
   auto kernelName = generateKernelNameConstant(
       launchOp.getKernelModuleName(), launchOp.getKernelName(), loc, rewriter);
   auto function = moduleGetFunctionCallBuilder.create(
-      loc, rewriter, {module.getResult(0), kernelName});
+      loc, rewriter, indexBitwidth, {module.getResult(0), kernelName});
   auto zero = rewriter.create<LLVM::ConstantOp>(loc, llvmInt32Type,
                                                 rewriter.getI32IntegerAttr(0));
   auto adaptor =
       gpu::LaunchFuncOpAdaptor(operands, launchOp->getAttrDictionary());
   Value stream =
       adaptor.asyncDependencies().empty()
-          ? streamCreateCallBuilder.create(loc, rewriter, {}).getResult(0)
+          ? streamCreateCallBuilder.create(loc, rewriter, indexBitwidth, {})
+                .getResult(0)
           : adaptor.asyncDependencies().front();
   // Create array of pointers to kernel arguments.
   auto kernelParams = generateParamsArray(launchOp, operands, rewriter);
   auto nullpointer = rewriter.create<LLVM::NullOp>(loc, llvmPointerPointerType);
-  launchKernelCallBuilder.create(loc, rewriter,
+  launchKernelCallBuilder.create(loc, rewriter, indexBitwidth,
                                  {function.getResult(0), launchOp.gridSizeX(),
                                   launchOp.gridSizeY(), launchOp.gridSizeZ(),
                                   launchOp.blockSizeX(), launchOp.blockSizeY(),
@@ -655,11 +706,12 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
     // Synchronize with host and destroy stream. This must be the stream created
     // above (with no other uses) because we check that the synchronous version
     // does not have any async dependencies.
-    streamSynchronizeCallBuilder.create(loc, rewriter, stream);
-    streamDestroyCallBuilder.create(loc, rewriter, stream);
+    streamSynchronizeCallBuilder.create(loc, rewriter, indexBitwidth, stream);
+    streamDestroyCallBuilder.create(loc, rewriter, indexBitwidth, stream);
     rewriter.eraseOp(launchOp);
   }
-  moduleUnloadCallBuilder.create(loc, rewriter, module.getResult(0));
+  moduleUnloadCallBuilder.create(loc, rewriter, indexBitwidth,
+                                 module.getResult(0));
 
   return success();
 }
@@ -675,6 +727,7 @@ LogicalResult ConvertMemcpyOpToGpuRuntimeCallPattern::matchAndRewrite(
     return failure();
 
   auto loc = memcpyOp.getLoc();
+  unsigned indexBitwidth = getTypeConverter()->getIndexTypeBitwidth();
   auto adaptor = gpu::MemcpyOpAdaptor(operands, memcpyOp->getAttrDictionary());
 
   MemRefDescriptor srcDesc(adaptor.src());
@@ -701,7 +754,8 @@ LogicalResult ConvertMemcpyOpToGpuRuntimeCallPattern::matchAndRewrite(
       MemRefDescriptor(adaptor.dst()).alignedPtr(rewriter, loc));
 
   auto stream = adaptor.asyncDependencies().front();
-  memcpyCallBuilder.create(loc, rewriter, {dst, src, sizeBytes, stream});
+  memcpyCallBuilder.create(loc, rewriter, indexBitwidth,
+                           {dst, src, sizeBytes, stream});
 
   rewriter.replaceOp(memcpyOp, {stream});
 
@@ -709,8 +763,11 @@ LogicalResult ConvertMemcpyOpToGpuRuntimeCallPattern::matchAndRewrite(
 }
 
 std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
-mlir::createGpuToLLVMConversionPass(StringRef gpuBinaryAnnotation) {
-  return std::make_unique<GpuToLLVMConversionPass>(gpuBinaryAnnotation);
+mlir::createGpuToLLVMConversionPass(StringRef gpuBinaryAnnotation,
+                                    const LowerToLLVMOptions &options) {
+  return std::make_unique<GpuToLLVMConversionPass>(
+      gpuBinaryAnnotation, options.useBarePtrCallConv, options.emitCWrappers,
+      options.indexBitwidth, options.useAlignedAlloc, options.dataLayout);
 }
 
 void mlir::populateGpuToLLVMConversionPatterns(
diff --git a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
index 4f62f204f4a8..b8f55ddd8e83 100644
--- a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
@@ -36,7 +36,7 @@ static auto InitializeCtx = [] {
   HIP_REPORT_IF_ERROR(hipInit(/*flags=*/0));
   hipDevice_t device;
   HIP_REPORT_IF_ERROR(hipDeviceGet(&device, /*ordinal=*/0));
-  hipContext_t context;
+  hipCtx_t context;
   HIP_REPORT_IF_ERROR(hipCtxCreate(&context, /*flags=*/0, device));
   return 0;
 }();
@@ -110,17 +110,17 @@ extern "C" void mgpuEventRecord(hipEvent_t event, hipStream_t stream) {
 
 extern "C" void *mgpuMemAlloc(uint64_t sizeBytes, hipStream_t /*stream*/) {
   void *ptr;
-  HIP_REPORT_IF_ERROR(hipMemAlloc(&ptr, sizeBytes));
+  HIP_REPORT_IF_ERROR(hipMalloc(&ptr, sizeBytes));
   return ptr;
 }
 
 extern "C" void mgpuMemFree(void *ptr, hipStream_t /*stream*/) {
-  HIP_REPORT_IF_ERROR(hipMemFree(ptr));
+  HIP_REPORT_IF_ERROR(hipFree(ptr));
 }
 
 extern "C" void mgpuMemcpy(void *dst, void *src, uint64_t sizeBytes,
                            hipStream_t stream) {
-  HIP_REPORT_IF_ERROR(hipMemcpyAsync(dst, src, sizeBytes, stream));
+  HIP_REPORT_IF_ERROR(hipMemcpyAsync(dst, src, sizeBytes, hipMemcpyDefault, stream));
 }
 
 /// Helper functions for writing mlir example code
