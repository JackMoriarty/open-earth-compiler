diff --git a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
index 51c4cc924fc..738462d6807 100644
--- a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
+++ b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
@@ -101,12 +101,12 @@ protected:
       llvmVoidType,
       {
           llvmPointerType,        /* void* f */
-          llvmIntPtrType,         /* intptr_t gridXDim */
-          llvmIntPtrType,         /* intptr_t gridyDim */
-          llvmIntPtrType,         /* intptr_t gridZDim */
-          llvmIntPtrType,         /* intptr_t blockXDim */
-          llvmIntPtrType,         /* intptr_t blockYDim */
-          llvmIntPtrType,         /* intptr_t blockZDim */
+          llvmInt32Type,          /* unsigned int gridXDim */
+          llvmInt32Type,          /* unsigned int gridyDim */
+          llvmInt32Type,          /* unsigned int gridZDim */
+          llvmInt32Type,          /* unsigned int blockXDim */
+          llvmInt32Type,          /* unsigned int blockYDim */
+          llvmInt32Type,          /* unsigned int blockZDim */
           llvmInt32Type,          /* unsigned int sharedMemBytes */
           llvmPointerType,        /* void *hstream */
           llvmPointerPointerType, /* void **kernelParams */
@@ -118,6 +118,37 @@ protected:
       "mgpuStreamSynchronize",
       llvmVoidType,
       {llvmPointerType /* void *stream */}};
+  FunctionCallBuilder memAllocCallBuilder = {
+      "mgpuMemAlloc",
+      llvmVoidType,
+      {
+          llvmPointerPointerType, /* void **ptr */
+          llvmInt64Type /* int64 sizeBytes */
+      }};
+  FunctionCallBuilder memFreeCallBuilder = {
+      "mgpuMemFree",
+      llvmVoidType,
+      {llvmPointerType  /* void *ptr */ }};
+
+  // Truncate an index value type to unsigned int if necessary.
+  Value truncateIndex(OpBuilder &builder, Location loc, Value value) const {
+    auto llvmType = value.getType().cast<LLVM::LLVMType>();
+    if (llvmType.getIntegerBitWidth() > 32) {
+      return builder.create<LLVM::TruncOp>(loc, llvmInt32Type, value);
+    }
+    return value;
+  }
+
+  // Extend an index value type to a size type if necessary,
+  Value extendIndex(OpBuilder &builder, Location loc, Value value) const {
+    auto llvmType = value.getType().cast<LLVM::LLVMType>();
+    if (llvmType.getIntegerBitWidth() < 64) {
+      return builder.create<LLVM::ZExtOp>(loc, llvmInt64Type, value);
+    }
+    return value;
+  }
+
+
 };
 
 /// A rewrite patter to convert gpu.launch_func operations into a sequence of
@@ -168,8 +199,20 @@ class EraseGpuModuleOpPattern : public OpRewritePattern<gpu::GPUModuleOp> {
   }
 };
 
+class ConvertAllocFreeOpsToGpuRuntimeAllocFreePattern
+    : public ConvertOpToGpuRuntimeCallPattern<LLVM::CallOp> {
+public:
+  ConvertAllocFreeOpsToGpuRuntimeAllocFreePattern(LLVMTypeConverter &typeConverter)
+      : ConvertOpToGpuRuntimeCallPattern<LLVM::CallOp>(typeConverter) {}
+
+private:
+  LogicalResult
+  matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+                  ConversionPatternRewriter &rewriter) const override;
+};
 } // namespace
 
+
 void GpuLaunchFuncToGpuRuntimeCallsPass::runOnOperation() {
   LLVMTypeConverter converter(&getContext());
   OwningRewritePatternList patterns;
@@ -367,13 +410,21 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
   // Invoke the function with required arguments.
   auto zero = rewriter.create<LLVM::ConstantOp>(loc, llvmInt32Type,
                                                 rewriter.getI32IntegerAttr(0));
+
+  auto gridSizeX = truncateIndex(rewriter, loc, launchOp.gridSizeX());
+  auto gridSizeY = truncateIndex(rewriter, loc, launchOp.gridSizeY());
+  auto gridSizeZ = truncateIndex(rewriter, loc, launchOp.gridSizeZ());
+  auto blockSizeX = truncateIndex(rewriter, loc, launchOp.blockSizeX());
+  auto blockSizeY = truncateIndex(rewriter, loc, launchOp.blockSizeY());
+  auto blockSizeZ = truncateIndex(rewriter, loc, launchOp.blockSizeZ());
+
   auto nullpointer =
       rewriter.create<LLVM::IntToPtrOp>(loc, llvmPointerPointerType, zero);
   launchKernelCallBuilder.create(
       loc, rewriter,
-      {function.getResult(0), launchOp.gridSizeX(), launchOp.gridSizeY(),
-       launchOp.gridSizeZ(), launchOp.blockSizeX(), launchOp.blockSizeY(),
-       launchOp.blockSizeZ(), zero, /* sharedMemBytes */
+      {function.getResult(0), gridSizeX, gridSizeY,
+       gridSizeZ, blockSizeX, blockSizeY,
+       blockSizeZ, zero, /* sharedMemBytes */
        stream.getResult(0),         /* stream */
        kernelParams,                /* kernel params */
        nullpointer /* extra */});
@@ -383,6 +434,34 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
   return success();
 }
 
+LogicalResult ConvertAllocFreeOpsToGpuRuntimeAllocFreePattern::matchAndRewrite(
+    Operation *op, ArrayRef<Value> operands,
+    ConversionPatternRewriter &rewriter) const {
+  auto callOp = cast<LLVM::CallOp>(op);
+  // Replace all memory allocations by GPU memory allocations or frees.
+  if (callOp.callee().getValueOr("") == "malloc" ||
+      callOp.callee().getValueOr("") == "free") {
+    OpBuilder builder(callOp);
+    Location loc = callOp.getLoc();
+
+    if (callOp.callee().getValue() == "malloc") {
+      auto one = builder.create<LLVM::ConstantOp>(loc, llvmInt64Type,
+                                                  builder.getI64IntegerAttr(1));
+      auto allocPtr =
+          builder.create<LLVM::AllocaOp>(loc, llvmPointerPointerType, one, 0);
+      auto size = extendIndex(builder, loc, callOp.getOperand(0));
+      memAllocCallBuilder.create(loc, builder, ArrayRef<Value>{allocPtr, size});
+      callOp.getResult(0).replaceAllUsesWith(
+          builder.create<LLVM::LoadOp>(loc, llvmPointerType, allocPtr));
+    }
+    if (callOp.callee().getValue() == "free") {
+      memFreeCallBuilder.create(loc, builder,
+                                         ArrayRef<Value>{callOp.getOperand(0)});
+    }
+    callOp.erase();
+  }
+}
+
 std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
 mlir::createConvertGpuLaunchFuncToGpuRuntimeCallsPass(
     StringRef gpuBinaryAnnotation) {
@@ -395,5 +474,7 @@ void mlir::populateGpuToLLVMConversionPatterns(
     StringRef gpuBinaryAnnotation) {
   patterns.insert<ConvertLaunchFuncOpToGpuRuntimeCallPattern>(
       converter, gpuBinaryAnnotation);
+  patterns.insert<ConvertAllocFreeOpsToGpuRuntimeAllocFreePattern>(
+      converter);
   patterns.insert<EraseGpuModuleOpPattern>(&converter.getContext());
 }
diff --git a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
index 8e2dc029fa9..8606373737a 100644
--- a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
@@ -17,6 +17,7 @@
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
 #include "llvm/Support/raw_ostream.h"
 
 #include "cuda.h"
@@ -32,6 +33,74 @@
     llvm::errs() << "'" << #expr << "' failed with '" << name << "'\n";        \
   }(expr)
 
+namespace {
+// Context object that buffers GPU modules, functions, temporary storage.
+struct Runtime {
+  // Load a module and cache it.
+  void loadModule(CUmodule *module, void *data) {
+    int32_t err = CUDA_SUCCESS;
+    // Load the module during the first execution.
+    if(moduleList.count(data) == 0) {
+      CUDA_REPORT_IF_ERROR(cuModuleLoadData(module, data));
+      moduleList[data] = *module;
+    }
+    *module = moduleList[data];
+  }
+
+  // Get a function an cache it.
+  void getFunction(CUfunction *function, CUmodule module, const char *name) {
+    // Get the function during the first execution.
+    if(functionList.count(name) == 0) {
+      CUDA_REPORT_IF_ERROR(cuModuleGetFunction(function, module, name));
+      functionList[name] = *function;
+    }
+    *function = functionList[name];
+  }
+
+  // Get the default stream.
+  void createStream(CUstream *stream) {
+    if(streamList.empty()) {
+      CUstream stream;
+      CUDA_REPORT_IF_ERROR(cuStreamCreate(&stream, CU_STREAM_DEFAULT));
+      streamList.push_back(stream);
+    }
+    *stream = streamList.back();
+  }
+
+  // Allocate GPU device memory.
+  void allocMem(CUdeviceptr *ptr, size_t size) {
+    // Allocate storage if free list contains no matching allocation.
+    if(tempList.count(size) == 0 || tempList[size].empty()) {
+      CUDA_REPORT_IF_ERROR(cuMemAlloc(ptr, size));
+      return;
+    }
+    // Return existing allocation.
+    *ptr = tempList[size].back();
+    tempList[size].pop_back();
+  }
+
+  // Free GPU device memory.
+  void freeMem(CUdeviceptr ptr) {
+    CUdeviceptr allocPtr;
+    size_t allocSize = 0;
+    // Get the size of the allocation.
+    CUDA_REPORT_IF_ERROR(cuMemGetAddressRange(&allocPtr, &allocSize, ptr));
+    tempList[allocSize].push_back(ptr);
+  }
+
+  static Runtime &getInstance() {
+    thread_local Runtime runtime;
+    return runtime;
+  }
+
+private:
+  std::vector<CUstream> streamList;
+  llvm::DenseMap<void*, CUmodule> moduleList;
+  llvm::DenseMap<const char*, CUfunction> functionList;
+  llvm::DenseMap<size_t, std::vector<CUdeviceptr>> tempList;
+};
+} // anonymous namespace
+
 extern "C" CUmodule mgpuModuleLoad(void *data) {
   CUmodule module = nullptr;
   CUDA_REPORT_IF_ERROR(cuModuleLoadData(&module, data));
@@ -59,7 +128,7 @@ extern "C" void mgpuLaunchKernel(CUfunction function, intptr_t gridX,
 
 extern "C" CUstream mgpuStreamCreate() {
   CUstream stream = nullptr;
-  CUDA_REPORT_IF_ERROR(cuStreamCreate(&stream, CU_STREAM_NON_BLOCKING));
+  Runtime::getInstance().createStream(&stream);
   return stream;
 }
 
@@ -67,6 +136,14 @@ extern "C" void mgpuStreamSynchronize(CUstream stream) {
   CUDA_REPORT_IF_ERROR(cuStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(CUdeviceptr *ptr, uint64_t size) {
+  Runtime::getInstance().allocMem(ptr, size);
+}
+
+extern "C" void mgpuMemFree(CUdeviceptr ptr) {
+  Runtime::getInstance().freeMem(ptr);
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the CUDA runtime. Helpful until we have
diff --git a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
index a6481500766..5e0ddaa8ec5 100644
--- a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
@@ -17,6 +17,7 @@
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
 #include "llvm/Support/raw_ostream.h"
 
 #include "hip/hip_runtime.h"
@@ -31,16 +32,83 @@
     llvm::errs() << "'" << #expr << "' failed with '" << name << "'\n";        \
   }(expr)
 
+namespace {
+// Context object that buffers GPU modules, functions, temporary storage.
+struct Runtime {
+  // Load a module and cache it.
+  void loadModule(hipModule_t *module, void *data) {
+    // Load the module during the first execution.
+    if(moduleList.count(data) == 0) {
+      HIP_REPORT_IF_ERROR(hipModuleLoadData(module, data));
+      moduleList[data] = *module;
+    }
+    *module = moduleList[data];
+  }
+
+  // Get a function an cache it.
+  void getFunction(hipFunction_t *function, hipModule_t module, const char *name) {
+    // Get the function during the first execution.
+    if(functionList.count(name) == 0) {
+      HIP_REPORT_IF_ERROR(hipModuleGetFunction(function, module, name));
+      functionList[name] = *function;
+    }
+    *function = functionList[name];
+  }
+
+  // Get the default stream.
+  void createStream(hipStream_t *stream) {
+    if(streamList.empty()) {
+      hipStream_t stream;
+      HIP_REPORT_IF_ERROR(hipStreamCreate(&stream));
+      streamList.push_back(stream);
+    }
+    *stream = streamList.back();
+  }
+
+  // Allocate GPU device memory.
+  void allocMem(hipDeviceptr_t *ptr, size_t size) {
+    // Allocate storage if free list contains no matching allocation.
+    if(tempList.count(size) == 0 || tempList[size].empty()) {
+      HIP_REPORT_IF_ERROR(hipMalloc(ptr, size));
+      return;
+    }
+    // Return existing allocation.
+    *ptr = tempList[size].back();
+    tempList[size].pop_back();
+  }
+
+  // Free GPU device memory.
+  void freeMem(hipDeviceptr_t ptr) {
+    hipDeviceptr_t allocPtr;
+    size_t allocSize = 0;
+    // Get the size of the allocation.
+    HIP_REPORT_IF_ERROR(hipMemGetAddressRange(&allocPtr, &allocSize, ptr));
+    tempList[allocSize].push_back(ptr);
+  }
+
+  static Runtime &getInstance() {
+    thread_local Runtime runtime;
+    return runtime;
+  }
+
+private:
+  std::vector<hipStream_t> streamList;
+  llvm::DenseMap<void*, hipModule_t> moduleList;
+  llvm::DenseMap<const char*, hipFunction_t> functionList;
+  llvm::DenseMap<size_t, std::vector<hipDeviceptr_t>> tempList;
+};
+} // anonymous namespace
+
 extern "C" hipModule_t mgpuModuleLoad(void *data) {
   hipModule_t module = nullptr;
-  HIP_REPORT_IF_ERROR(hipModuleLoadData(&module, data));
+  Runtime::getInstance().loadModule(&module, data);
   return module;
 }
 
 extern "C" hipFunction_t mgpuModuleGetFunction(hipModule_t module,
                                                const char *name) {
   hipFunction_t function = nullptr;
-  HIP_REPORT_IF_ERROR(hipModuleGetFunction(&function, module, name));
+  Runtime::getInstance().getFunction(&function, module, name);
   return function;
 }
 
@@ -60,7 +128,7 @@ extern "C" void mgpuLaunchKernel(hipFunction_t function, intptr_t gridX,
 
 extern "C" void *mgpuStreamCreate() {
   hipStream_t stream = nullptr;
-  HIP_REPORT_IF_ERROR(hipStreamCreate(&stream));
+  Runtime::getInstance().createStream(&stream);
   return stream;
 }
 
@@ -68,6 +136,14 @@ extern "C" void mgpuStreamSynchronize(hipStream_t stream) {
   return HIP_REPORT_IF_ERROR(hipStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(hipDeviceptr_t *ptr, uint64_t size) {
+  Runtime::getInstance().allocMem(ptr, size);
+}
+
+extern "C" void mgpuMemFree(hipDeviceptr_t ptr) {
+  Runtime::getInstance().freeMem(ptr);
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the ROCM runtime. Helpful until we have
