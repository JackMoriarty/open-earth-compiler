diff --git a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
index 51c4cc924fc..262f5894200 100644
--- a/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
+++ b/mlir/lib/Conversion/GPUCommon/ConvertLaunchFuncToRuntimeCalls.cpp
@@ -42,6 +42,19 @@ namespace {
 class GpuLaunchFuncToGpuRuntimeCallsPass
     : public ConvertGpuLaunchFuncToGpuRuntimeCallsBase<
           GpuLaunchFuncToGpuRuntimeCallsPass> {
+  // Extend an index value type to a size type if necessary,
+  Value extendIndex(OpBuilder &builder, Location loc, Value value) const {
+    LLVM::LLVMType llvmInt64Type =
+        LLVM::LLVMType::getInt64Ty(builder.getContext());
+    auto llvmType = value.getType().cast<LLVM::LLVMType>();
+    if (llvmType.getIntegerBitWidth() < 64) {
+      return builder.create<LLVM::ZExtOp>(loc, llvmInt64Type, value);
+    }
+    return value;
+  }
+
+  void replaceMallocAndFreeCalls(LLVM::CallOp callOp);
+
 public:
   GpuLaunchFuncToGpuRuntimeCallsPass(StringRef gpuBinaryAnnotation) {
     if (!gpuBinaryAnnotation.empty())
@@ -101,12 +114,12 @@ protected:
       llvmVoidType,
       {
           llvmPointerType,        /* void* f */
-          llvmIntPtrType,         /* intptr_t gridXDim */
-          llvmIntPtrType,         /* intptr_t gridyDim */
-          llvmIntPtrType,         /* intptr_t gridZDim */
-          llvmIntPtrType,         /* intptr_t blockXDim */
-          llvmIntPtrType,         /* intptr_t blockYDim */
-          llvmIntPtrType,         /* intptr_t blockZDim */
+          llvmInt32Type,          /* unsigned int gridXDim */
+          llvmInt32Type,          /* unsigned int gridyDim */
+          llvmInt32Type,          /* unsigned int gridZDim */
+          llvmInt32Type,          /* unsigned int blockXDim */
+          llvmInt32Type,          /* unsigned int blockYDim */
+          llvmInt32Type,          /* unsigned int blockZDim */
           llvmInt32Type,          /* unsigned int sharedMemBytes */
           llvmPointerType,        /* void *hstream */
           llvmPointerPointerType, /* void **kernelParams */
@@ -118,6 +131,17 @@ protected:
       "mgpuStreamSynchronize",
       llvmVoidType,
       {llvmPointerType /* void *stream */}};
+
+  // Truncate an index value type to unsigned int if necessary.
+  Value truncateIndex(OpBuilder &builder, Location loc, Value value) const {
+    LLVM::LLVMType llvmInt32Type =
+        LLVM::LLVMType::getInt32Ty(builder.getContext());
+    auto llvmType = value.getType().cast<LLVM::LLVMType>();
+    if (llvmType.getIntegerBitWidth() > 32) {
+      return builder.create<LLVM::TruncOp>(loc, llvmInt32Type, value);
+    }
+    return value;
+  }
 };
 
 /// A rewrite patter to convert gpu.launch_func operations into a sequence of
@@ -178,6 +202,90 @@ void GpuLaunchFuncToGpuRuntimeCallsPass::runOnOperation() {
   LLVMConversionTarget target(getContext());
   if (failed(applyPartialConversion(getOperation(), target, patterns)))
     signalPassFailure();
+
+  // Allocate and free device memory instead of host memory.
+  getOperation().walk(
+      [this](LLVM::CallOp op) { replaceMallocAndFreeCalls(op); });
+
+  // Erase the malloc and free function declarations if they are unused.
+  if (llvm::none_of(getOperation().getOps<LLVM::CallOp>(),
+                    [](LLVM::CallOp callOp) {
+                      return callOp.callee().getValueOr("") == "malloc" ||
+                             callOp.callee().getValueOr("") == "free";
+                    })) {
+    if (auto *malloc = getOperation().lookupSymbol("malloc"))
+      malloc->erase();
+    if (auto *free = getOperation().lookupSymbol("free"))
+      free->erase();
+  }
+}
+
+// Replace all malloc and free pairs by GPU memory allocations.
+void GpuLaunchFuncToGpuRuntimeCallsPass::replaceMallocAndFreeCalls(
+    LLVM::CallOp callOp) {
+
+  // Prepare the types
+  LLVM::LLVMType llvmVoidType = LLVM::LLVMType::getVoidTy(callOp.getContext());
+  LLVM::LLVMType llvmPointerType =
+      LLVM::LLVMType::getInt8PtrTy(callOp.getContext());
+  LLVM::LLVMType llvmPointerPointerType = llvmPointerType.getPointerTo();
+  LLVM::LLVMType llvmInt64Type =
+      LLVM::LLVMType::getInt64Ty(callOp.getContext());
+
+  // Replace all memory allocations by GPU memory allocations or frees.
+  if (callOp.callee().getValueOr("") == "malloc" ||
+      callOp.callee().getValueOr("") == "free") {
+    OpBuilder builder(callOp);
+    Location loc = callOp.getLoc();
+
+    if (!getOperation().lookupSymbol("mgpuMemAlloc")) {
+      auto module =
+          builder.getBlock()->getParent()->getParentOfType<ModuleOp>();
+      OpBuilder(module.getBody()->getTerminator())
+          .create<LLVM::LLVMFuncOp>(
+              loc, "mgpuMemAlloc",
+              LLVM::LLVMType::getFunctionTy(
+                  llvmVoidType,
+                  {
+                      llvmPointerPointerType, /* void **ptr */
+                      llvmInt64Type           /* int64 sizeBytes */
+                  },
+                  /*isVarArg=*/false));
+    }
+    if (!getOperation().lookupSymbol("mgpuMemFree")) {
+      auto module =
+          builder.getBlock()->getParent()->getParentOfType<ModuleOp>();
+      OpBuilder(module.getBody()->getTerminator())
+          .create<LLVM::LLVMFuncOp>(
+              loc, "mgpuMemFree",
+              LLVM::LLVMType::getFunctionTy(llvmVoidType,
+                                            llvmPointerType, /* void *ptr */
+                                            /*isVarArg=*/false));
+    }
+
+    if (callOp.callee().getValue() == "malloc") {
+      auto one = builder.create<LLVM::ConstantOp>(loc, llvmInt64Type,
+                                                  builder.getI64IntegerAttr(1));
+      auto allocPtr =
+          builder.create<LLVM::AllocaOp>(loc, llvmPointerPointerType, one, 0);
+      auto allocFunc =
+          getOperation().lookupSymbol<LLVM::LLVMFuncOp>("mgpuMemAlloc");
+      auto size = extendIndex(builder, loc, callOp.getOperand(0));
+      builder.create<LLVM::CallOp>(loc, ArrayRef<Type>{},
+                                   builder.getSymbolRefAttr(allocFunc),
+                                   ArrayRef<Value>{allocPtr, size});
+      callOp.getResult(0).replaceAllUsesWith(
+          builder.create<LLVM::LoadOp>(loc, llvmPointerType, allocPtr));
+    }
+    if (callOp.callee().getValue() == "free") {
+      auto freeFunc =
+          getOperation().lookupSymbol<LLVM::LLVMFuncOp>("mgpuMemFree");
+      builder.create<LLVM::CallOp>(loc, ArrayRef<Type>{},
+                                   builder.getSymbolRefAttr(freeFunc),
+                                   ArrayRef<Value>{callOp.getOperand(0)});
+    }
+    callOp.erase();
+  }
 }
 
 LLVM::CallOp FunctionCallBuilder::create(Location loc, OpBuilder &builder,
@@ -367,16 +475,23 @@ LogicalResult ConvertLaunchFuncOpToGpuRuntimeCallPattern::matchAndRewrite(
   // Invoke the function with required arguments.
   auto zero = rewriter.create<LLVM::ConstantOp>(loc, llvmInt32Type,
                                                 rewriter.getI32IntegerAttr(0));
+
+  auto gridSizeX = truncateIndex(rewriter, loc, launchOp.gridSizeX());
+  auto gridSizeY = truncateIndex(rewriter, loc, launchOp.gridSizeY());
+  auto gridSizeZ = truncateIndex(rewriter, loc, launchOp.gridSizeZ());
+  auto blockSizeX = truncateIndex(rewriter, loc, launchOp.blockSizeX());
+  auto blockSizeY = truncateIndex(rewriter, loc, launchOp.blockSizeY());
+  auto blockSizeZ = truncateIndex(rewriter, loc, launchOp.blockSizeZ());
+
   auto nullpointer =
       rewriter.create<LLVM::IntToPtrOp>(loc, llvmPointerPointerType, zero);
-  launchKernelCallBuilder.create(
-      loc, rewriter,
-      {function.getResult(0), launchOp.gridSizeX(), launchOp.gridSizeY(),
-       launchOp.gridSizeZ(), launchOp.blockSizeX(), launchOp.blockSizeY(),
-       launchOp.blockSizeZ(), zero, /* sharedMemBytes */
-       stream.getResult(0),         /* stream */
-       kernelParams,                /* kernel params */
-       nullpointer /* extra */});
+  launchKernelCallBuilder.create(loc, rewriter,
+                                 {function.getResult(0), gridSizeX, gridSizeY,
+                                  gridSizeZ, blockSizeX, blockSizeY, blockSizeZ,
+                                  zero,                /* sharedMemBytes */
+                                  stream.getResult(0), /* stream */
+                                  kernelParams,        /* kernel params */
+                                  nullpointer /* extra */});
   streamSynchronizeCallBuilder.create(loc, rewriter, stream.getResult(0));
 
   rewriter.eraseOp(op);
diff --git a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
index 8e2dc029fa9..8606373737a 100644
--- a/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-cuda-runner/cuda-runtime-wrappers.cpp
@@ -17,6 +17,7 @@
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
 #include "llvm/Support/raw_ostream.h"
 
 #include "cuda.h"
@@ -32,6 +33,74 @@
     llvm::errs() << "'" << #expr << "' failed with '" << name << "'\n";        \
   }(expr)
 
+namespace {
+// Context object that buffers GPU modules, functions, temporary storage.
+struct Runtime {
+  // Load a module and cache it.
+  void loadModule(CUmodule *module, void *data) {
+    int32_t err = CUDA_SUCCESS;
+    // Load the module during the first execution.
+    if(moduleList.count(data) == 0) {
+      CUDA_REPORT_IF_ERROR(cuModuleLoadData(module, data));
+      moduleList[data] = *module;
+    }
+    *module = moduleList[data];
+  }
+
+  // Get a function an cache it.
+  void getFunction(CUfunction *function, CUmodule module, const char *name) {
+    // Get the function during the first execution.
+    if(functionList.count(name) == 0) {
+      CUDA_REPORT_IF_ERROR(cuModuleGetFunction(function, module, name));
+      functionList[name] = *function;
+    }
+    *function = functionList[name];
+  }
+
+  // Get the default stream.
+  void createStream(CUstream *stream) {
+    if(streamList.empty()) {
+      CUstream stream;
+      CUDA_REPORT_IF_ERROR(cuStreamCreate(&stream, CU_STREAM_DEFAULT));
+      streamList.push_back(stream);
+    }
+    *stream = streamList.back();
+  }
+
+  // Allocate GPU device memory.
+  void allocMem(CUdeviceptr *ptr, size_t size) {
+    // Allocate storage if free list contains no matching allocation.
+    if(tempList.count(size) == 0 || tempList[size].empty()) {
+      CUDA_REPORT_IF_ERROR(cuMemAlloc(ptr, size));
+      return;
+    }
+    // Return existing allocation.
+    *ptr = tempList[size].back();
+    tempList[size].pop_back();
+  }
+
+  // Free GPU device memory.
+  void freeMem(CUdeviceptr ptr) {
+    CUdeviceptr allocPtr;
+    size_t allocSize = 0;
+    // Get the size of the allocation.
+    CUDA_REPORT_IF_ERROR(cuMemGetAddressRange(&allocPtr, &allocSize, ptr));
+    tempList[allocSize].push_back(ptr);
+  }
+
+  static Runtime &getInstance() {
+    thread_local Runtime runtime;
+    return runtime;
+  }
+
+private:
+  std::vector<CUstream> streamList;
+  llvm::DenseMap<void*, CUmodule> moduleList;
+  llvm::DenseMap<const char*, CUfunction> functionList;
+  llvm::DenseMap<size_t, std::vector<CUdeviceptr>> tempList;
+};
+} // anonymous namespace
+
 extern "C" CUmodule mgpuModuleLoad(void *data) {
   CUmodule module = nullptr;
   CUDA_REPORT_IF_ERROR(cuModuleLoadData(&module, data));
@@ -59,7 +128,7 @@ extern "C" void mgpuLaunchKernel(CUfunction function, intptr_t gridX,
 
 extern "C" CUstream mgpuStreamCreate() {
   CUstream stream = nullptr;
-  CUDA_REPORT_IF_ERROR(cuStreamCreate(&stream, CU_STREAM_NON_BLOCKING));
+  Runtime::getInstance().createStream(&stream);
   return stream;
 }
 
@@ -67,6 +136,14 @@ extern "C" void mgpuStreamSynchronize(CUstream stream) {
   CUDA_REPORT_IF_ERROR(cuStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(CUdeviceptr *ptr, uint64_t size) {
+  Runtime::getInstance().allocMem(ptr, size);
+}
+
+extern "C" void mgpuMemFree(CUdeviceptr ptr) {
+  Runtime::getInstance().freeMem(ptr);
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the CUDA runtime. Helpful until we have
diff --git a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
index a6481500766..5e0ddaa8ec5 100644
--- a/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
+++ b/mlir/tools/mlir-rocm-runner/rocm-runtime-wrappers.cpp
@@ -17,6 +17,7 @@
 
 #include "mlir/ExecutionEngine/CRunnerUtils.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
 #include "llvm/Support/raw_ostream.h"
 
 #include "hip/hip_runtime.h"
@@ -31,16 +32,83 @@
     llvm::errs() << "'" << #expr << "' failed with '" << name << "'\n";        \
   }(expr)
 
+namespace {
+// Context object that buffers GPU modules, functions, temporary storage.
+struct Runtime {
+  // Load a module and cache it.
+  void loadModule(hipModule_t *module, void *data) {
+    // Load the module during the first execution.
+    if(moduleList.count(data) == 0) {
+      HIP_REPORT_IF_ERROR(hipModuleLoadData(module, data));
+      moduleList[data] = *module;
+    }
+    *module = moduleList[data];
+  }
+
+  // Get a function an cache it.
+  void getFunction(hipFunction_t *function, hipModule_t module, const char *name) {
+    // Get the function during the first execution.
+    if(functionList.count(name) == 0) {
+      HIP_REPORT_IF_ERROR(hipModuleGetFunction(function, module, name));
+      functionList[name] = *function;
+    }
+    *function = functionList[name];
+  }
+
+  // Get the default stream.
+  void createStream(hipStream_t *stream) {
+    if(streamList.empty()) {
+      hipStream_t stream;
+      HIP_REPORT_IF_ERROR(hipStreamCreate(&stream));
+      streamList.push_back(stream);
+    }
+    *stream = streamList.back();
+  }
+
+  // Allocate GPU device memory.
+  void allocMem(hipDeviceptr_t *ptr, size_t size) {
+    // Allocate storage if free list contains no matching allocation.
+    if(tempList.count(size) == 0 || tempList[size].empty()) {
+      HIP_REPORT_IF_ERROR(hipMalloc(ptr, size));
+      return;
+    }
+    // Return existing allocation.
+    *ptr = tempList[size].back();
+    tempList[size].pop_back();
+  }
+
+  // Free GPU device memory.
+  void freeMem(hipDeviceptr_t ptr) {
+    hipDeviceptr_t allocPtr;
+    size_t allocSize = 0;
+    // Get the size of the allocation.
+    HIP_REPORT_IF_ERROR(hipMemGetAddressRange(&allocPtr, &allocSize, ptr));
+    tempList[allocSize].push_back(ptr);
+  }
+
+  static Runtime &getInstance() {
+    thread_local Runtime runtime;
+    return runtime;
+  }
+
+private:
+  std::vector<hipStream_t> streamList;
+  llvm::DenseMap<void*, hipModule_t> moduleList;
+  llvm::DenseMap<const char*, hipFunction_t> functionList;
+  llvm::DenseMap<size_t, std::vector<hipDeviceptr_t>> tempList;
+};
+} // anonymous namespace
+
 extern "C" hipModule_t mgpuModuleLoad(void *data) {
   hipModule_t module = nullptr;
-  HIP_REPORT_IF_ERROR(hipModuleLoadData(&module, data));
+  Runtime::getInstance().loadModule(&module, data);
   return module;
 }
 
 extern "C" hipFunction_t mgpuModuleGetFunction(hipModule_t module,
                                                const char *name) {
   hipFunction_t function = nullptr;
-  HIP_REPORT_IF_ERROR(hipModuleGetFunction(&function, module, name));
+  Runtime::getInstance().getFunction(&function, module, name);
   return function;
 }
 
@@ -60,7 +128,7 @@ extern "C" void mgpuLaunchKernel(hipFunction_t function, intptr_t gridX,
 
 extern "C" void *mgpuStreamCreate() {
   hipStream_t stream = nullptr;
-  HIP_REPORT_IF_ERROR(hipStreamCreate(&stream));
+  Runtime::getInstance().createStream(&stream);
   return stream;
 }
 
@@ -68,6 +136,14 @@ extern "C" void mgpuStreamSynchronize(hipStream_t stream) {
   return HIP_REPORT_IF_ERROR(hipStreamSynchronize(stream));
 }
 
+extern "C" void mgpuMemAlloc(hipDeviceptr_t *ptr, uint64_t size) {
+  Runtime::getInstance().allocMem(ptr, size);
+}
+
+extern "C" void mgpuMemFree(hipDeviceptr_t ptr) {
+  Runtime::getInstance().freeMem(ptr);
+}
+
 /// Helper functions for writing mlir example code
 
 // Allows to register byte array with the ROCM runtime. Helpful until we have
